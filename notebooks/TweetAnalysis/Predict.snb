{
  "metadata" : {
    "name" : "Predict",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/home/automaton/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3", "- org.apache.spark % spark-core_2.10 % _", "com.databricks:spark-csv_2.10:1.2.0", "- org.apache.hadoop % _ % _", "org.twitter4j:twitter4j-core:3.0.3", "com.google.code.gson:gson:2.3", "org.apache.spark:spark-streaming-twitter_2.10:1.4.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.cassandra.connection.host" : "172.31.21.172",
      "spark.master" : "spark://172.31.21.172:7077",
      "spark.executor.cores" : "2",
      "spark.executor.memory" : "4G",
      "spark.cores.max" : "5",
      "spark.eventLog.enabled" : "true",
      "spark.eventLog.dir" : "logs/spark"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Setup the SQL Context and necessary imports"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "output_stream_collapsed" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport com.datastax.spark.connector.cql.CassandraConnector\n\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.streaming._\n\nimport java.util.concurrent.atomic.AtomicInteger\n\nimport com.google.gson.Gson\nimport org.apache.spark.streaming.twitter.TwitterUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport com.datastax.spark.connector._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@7b4a1709\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._\nimport com.datastax.spark.connector.streaming._\nimport java.util.concurrent.atomic.AtomicInteger\nimport com.google.gson.Gson\nimport org.apache.spark.streaming.twitter.TwitterUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.{SparkConf, SparkContext}\nimport com.datastax.spark.connector._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.linalg.{Vector, SparseVector}\nimport org.apache.spark.mllib.feature.HashingTF\n\nval numFeatures = 1000",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.mllib.linalg.{Vector, SparseVector}\nimport org.apache.spark.mllib.feature.HashingTF\nnumFeatures: Int = 1000\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "1000"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val tf = new HashingTF(numFeatures)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "tf: org.apache.spark.mllib.feature.HashingTF = org.apache.spark.mllib.feature.HashingTF@7d5655e2\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.mllib.feature.HashingTF@7d5655e2"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "/**\n * Create feature vectors by turning each tweet into bigrams of characters (an n-gram model)\n * and then hashing those to a length-1000 feature vector that we can pass to MLlib.\n * This is a common way to decrease the number of features in a model while still\n * getting excellent accuracy (otherwise every pair of Unicode characters would\n * potentially be a feature).\n */\ndef featurize(s: String):Vector = tf.transform(s.sliding(2).toSeq)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "featurize: (s: String)org.apache.spark.mllib.linalg.Vector\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.collection.JavaConverters._\n\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\n\n//SomeColumns(\"vector_id\",\"size\", \"indices\",\"values\")\nval clusterVectorsDF = sqlContext.read.format(\"org.apache.spark.sql.cassandra\")\n  .options(Map(\"keyspace\" -> \"tweet_db\", \"table\" -> \"cluster_vectors\"))\n  .load()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import scala.collection.JavaConverters._\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nclusterVectorsDF: org.apache.spark.sql.DataFrame = [vector_id: int, values: string]\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val clusterCenters: Array[Vector] = clusterVectorsDF.collect().map(row => Vectors.parse(row.getString(1)))\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "output_stream_collapsed" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def $(s:String) = sys.env(s)\nSystem.setProperty(\"twitter4j.oauth.consumerKey\", $(\"TWITTER_CONSUMER_KEY\"))\nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", $(\"TWITTER_CONSUMER_SECRET\"))\nSystem.setProperty(\"twitter4j.oauth.accessToken\", $(\"TWITTER_ACCESS_TOKEN\"))\nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", $(\"TWITTER_ACCESS_TOKEN_SECRET\"))\n\"twitter settings done!\"",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val filters = Array(\"spark\", \"scala\", \"music\")",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "In case there is already a `StreamingContext` started (previous test), we stop it!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "StreamingContext.getActive.foreach(_.stop(false))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "@transient val ssc = new StreamingContext(sparkContext, Seconds(2))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "@transient val twitterStream = TwitterUtils.createStream(ssc, None)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "@transient val statuses = twitterStream.map(tweet => tweet.getText)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.clustering.KMeansModel\nval model = new KMeansModel(clusterCenters)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val scope = new java.io.Serializable {  \n  val m = model\n  \n  var clusterNumber = 15\n  \n  val ft = featurize _\n  \n  @transient val st = statuses\n  \n  @transient val filteredTweets = st.filter{ t => \n    model.predict(ft(t)) == clusterNumber\n  }\n  \n  @transient val allTweets = st.map {t => (model.predict(ft(t)), t)}\n}\n",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//scope.filteredTweets.print()\n\"print in the console if uncommented\"",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Creating reactive lists showing the incoming tweets and the predictions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val tweets = ul(20) // this creates an HTML <ul> that responds to appendAll call reactively\nval predictions = ul(20)\ntable(2, Seq(text(\"Tweets\"), text(\"Cluster '\"+scope.clusterNumber+\"'\"), tweets, predictions))",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Consuming the streams and reactively updating the lists above."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "scope.filteredTweets.foreachRDD(rdd => predictions.appendAll(rdd.take(10).map(_.toString)))\n//scope.allTweets.foreachRDD(rdd => predictions.appendAll(rdd.take(10).map(_.toString)))\n\nstatuses.foreachRDD(rdd => tweets.appendAll(rdd.take(10).map(_.toString)))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "ssc.start()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//ssc.stop()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}